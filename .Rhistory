scale_y_sqrt()
p3 <- data %>% ggplot(aes(`char_freq_$`, group=class, fill=class)) +
geom_density(alpha=.5) +
scale_x_sqrt() + scale_y_sqrt()
p4 <- data %>% ggplot(aes(class, capital_run_length_longest)) +
geom_jitter(col='gray') +
geom_boxplot(alpha=.5) +
scale_y_log10()
grid.arrange(p1, p2, p3, p4, ncol=2)
# 변수명의 특수문자 처리----
old_names <- names(data)
new_names <- make.names(names(data), unique=T) # 특수문자를 숫자로 바꾸어줌
cbind(old_names, new_names) [old_names != new_names,]
names(data) <- new_names
# splitting dataset----
set.seed(1606)
n <- nrow(data)
idx <- 1:n
training_idx <- sample(idx, n * .6)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .2)
test_idx <- setdiff(idx, validate_idx)
training <- data[training_idx, ]
validation <- data[validate_idx, ]
test <- data[test_idx, ]
# logistic regression----
data_lm_full <- glm(class ~., data=training, family = binomial)
summary(data_lm_full)
predict(data_lm_full, newdata = data[1:5, ], type='response')
# model evalidation----
y_obs <- as.numeric(as.character(validation$class))
yhat_lm <- predict(data_lm_full, newdata=validation, type='response')
pred_lm <- prediction(yhat_lm, y_obs)
plot(performance(pred_lm, 'tpr', 'fpr'))
performance(pred_lm, 'auc')@y.values[[1]]
binomial_deviance(y_obs, yhat_lm)
sqrt(mean((yi - yhat_i)^2))
# model evalidation----
mse <- function(yi, yhat_i){
sqrt(mean((yi - yhat_i)^2))
}
binomial_deviance <- function(y_obs, yhat){
epsilon = 0.0001
yhat = ifelse(yhat < epsilon, epsilon, yhat)
yhat = ifelse(yhat > 1-epsilon, 1-epsilon, yhat)
a = ifelse(y_obs==0, 0, y_obs * log(y_obs/yhat))
b = ifelse(y_obs==1, 0, (1-y_obs) * log((1-y_obs)/(1-yhat)))
return(2*sum(a + b))
}
y_obs <- as.numeric(as.character(validation$class))
yhat_lm <- predict(data_lm_full, newdata=validation, type='response')
pred_lm <- prediction(yhat_lm, y_obs)
plot(performance(pred_lm, 'tpr', 'fpr'))
performance(pred_lm, 'auc')@y.values[[1]]
binomial_deviance(y_obs, yhat_lm)
training
head(training)
predict(model, newdata=training, type='response')
predict(data_lm_full, newdata=training, type='response')
yhat_lm <- predict(data_lm_full, newdata=training, type='response')
yhat_lm_train <- predict(data_lm_full, newdata=training, type='response')
head(test)
yhat_lm_test <- predict(dala_lm_full, newdata=test, type='response')
yhat_lm_test <- predict(dala_lm_full, newdata=test, type='response')
yhat_lm_test <- predict(data_lm_full, newdata=test, type='response')
y_obs <- as.numeric(as.character(validation$class))
yhat_lm <- predict(data_lm_full, newdata=validation, type='response')
pred_lm <- prediction(yhat_lm, y_obs)
plot(performance(pred_lm, 'tpr', 'fpr'))
performance(pred_lm, 'auc')@y.values[[1]]
binomial_deviance(y_obs, yhat_lm)
abline(0,1)
yhat_lm_test
range(yhat_lm_test)
yhat_lm_test > .5
test$class
ifelse(yhat_lm_test > .5, 1, 0)
table(test$class, ifselse(yhat_lm_test, 1, 0))
table(test$class, ifelse(yhat_lm_test, 1, 0))
unique(test$class)
as.factor(test$class)
as.factor(ifelse(yhat_lm_test > .5, 1, 0))
yhat_lm <- as.factor(ifelse(yhat_lm_test > .5, 1, 0))
y_obs <- as.factor(test$class)
table(y_obs, yhat_lm))
table(y_obs, yhat_lm)
table(y_obs, yhat_lm) -> cm; print(cm)
confusionMatrix(yhat_lm, y_obs)
# 다항 로지스틱 회귀 : 예측하고자 하는 분류 여러개
library(nnet)
m <- multinom(Species ~., data=iris)
head(fitted(m))
predict(m, newdata = iris[c(1,51,101), ], type='class')
predict(m, newdata = iris[c(1,51,101), ], type='response')
predict(m, newdata = iris[c(1,51,101), ], type='probs')
# accuracy----
yhat <- predict(m, newdata=iris)
sum(yhat == iris$Species) / nrow(yhat)
sum(yhat == iris$Species) / length(yhat)
xtabs(~ yhat + iris$Species)
# 의사결정나무----
# 수량형 변수를 팩터로 변환, 의사결정나무에 적합
# loading breast cancer dataset
library(curl)
h <- new_handle(copypostfields = "moo=moomooo")
handle_setheaders(h,
"Content-Type" = "text/moo",
"Cache-Control" = "no-cache",
"User-Agent" = "A cow"
)
tmp <- tempfile()
curl_download('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', tmp, handle=h)
data <-read.csv(tmp, header=F)
feature_names <- c('radius', 'texture', 'perimeter', 'area', 'smoothness',
'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dim')
names(data) <-
c('id', 'class',
paste0('mean_', feature_names),
paste0('se_', feature_names),
paste0('worst_', feature_names))
glimpse(data)
# 의사결정나무----
# 수량형 변수를 팩터로 변환, 의사결정나무에 적합
# loading breast cancer dataset
BreastCancer
as.tibble(fread('./data/titanic3.csv', data.table = F)) -> titanic
glimpse(titanic)
# titanic data 가공하기
## data loading
as.tibble(fread('./data/titanic3.csv', data.table = F)) -> titanic
glimpse(titanic)
# var "sex" preprocessing as 0(female), 1(male)
as.factor(titanic$sex) -> titanic$sex
(as.numeric(titanic$sex) - 1) -> titanic$sex #female -> 0, male -> 1
glimpse(titanic)
# var "suvivied " preprocessing as factor with labels c("dead", "survived")
titanic$survived <- factor(titanic$survived, levels=c(0,1), labels=c('dead', 'survived'))
glimpse(titanic)
# var "embarked" preprocessing
unique(titanic$embarked)
titanic %>%
filter(embarked != "") -> titanic
titanic$embarked <- as.factor(titanic$embarked)
titanic$embarked <- as.numeric(titanic$embarked) # "S" : 3, "C" : 1, "Q" : 2
glimpse(titanic)
titanic %>%
select_if(is.numeric) %>%
select(-body) %>%
replace(is.na(.), 0) %>%
cbind(survived = titanic$survived) -> titanic_preprocessed
glimpse(titanic_preprocessed)
# 예측할 변수를 'survived'로 선택
levels(titanic$survived)
prop.table(table(titanic_preprocessed$survived))
write.csv(titanic_preprocessed, './data/titanic_preprocessed.csv')
read.cvs('./data/titanic_preprocessed.csv')
read.csv('./data/titanic_preprocessed.csv')
read.csv('./data/titanic_preprocessed.csv') -> titanic; head(titanic)
glimpse(titanic)
# splitting dataset
set.seed(2018)
n <- nrow(titanic)
idx <- 1:n
training_idx <- sample(idx, n * .6)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .2)
test_idx <- setdiff(idx, validate_idx)
training <- data[training_idx, ]
validation <- data[validate_idx, ]
test <- data[test_idx, ]
# splitting dataset
set.seed(2018)
n <- nrow(titanic)
idx <- 1:n
training_idx <- sample(idx, n * .6)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .2)
test_idx <- setdiff(idx, validate_idx)
titanic.train <- titanic[training_idx, ]
titanic.validation <- titanic[validate_idx, ]
titanic.test <- titanic[test_idx, ]
m <- rpart(survived ~ pclass + sex + age + sibsp + parch + fare + embarked, data=titanic.train)
plot(m)
text(m, cex=.8)
# model evaluation----
yhat <- predict(m, newdata = titanic.validation); head(yhat)
y_obs <- titanic.validation$survived; head(y_obs)
ifelse(predict(m, newsata=titanic.validation)$survived > .5, 1, 0)
predict(m, newdata = titanic.validation)
predict(m, newdata = titanic.validation)[, survived]
head(predict(m, newdata = titanic.validation))
class(predict(m, newdata = titanic.validation))
as.data.frame(predict(m, newdata = titanic.validation))
as.data.frame(predict(m, newdata = titanic.validation))$survived
ifelse(as.data.frame(predict(m, newdata = titanic.validation))$survived > .5, 1, 0)
ifelse(as.data.frame(predict(m, newdata = titanic.validation))$survived > .5, 1, 0) -> yhat
y_obs <- titanic.validation$survived; head(y_obs)
ifelse(as.data.frame(predict(m, newdata = titanic.validation))$survived > .5, 'suvived', 'dead') -> yhat
yhat <- as.factor(yhat)
yhat
y_obs <- titanic.validation$survived; head(y_obs)
confusionMatrix(yhat, y_obs)
yhat
class(yhat)
y_obs <- titanic.validation$survived; head(y_obs)
class(y_obs)
length(yhat)
length(y_obs)
confusionMatrix(yhat, y_obs)
levels(yaht)
levels(yhat)
levels(y_obs)
# model evaluation----
yhat <- predict(m, newdata = titanic.validation); head(yhat)
ifelse(as.data.frame(predict(m, newdata = titanic.validation))$survived > .5, 'survived', 'dead') -> yhat
yhat <- as.factor(yhat)
length(yhat)
levels(yhat)
y_obs <- titanic.validation$survived; head(y_obs)
length(y_obs)
levels(y_obs)
confusionMatrix(yhat, y_obs)
library(ROCR)
y_obs <- as.numeric(y_obs)
y_obs
y_obs <- as.numeric(y_obs) - 1
y_obs
yhat_lm <- as.data.frame(predict(m, newdata=titanic.validation))$survived
yhat_lm
pred_lm <- prediction(yhat_lm, y_obs)
plot(performance(pred_lm, 'tpr', 'fpr'))
abline(0,1)
performance(pred_lm, 'auc')@y.values[[1]]
# randomForest----
rf_fit <- train(survived ~ ., data=titanic.train,
preProcess = c("pca"),
method='rf', ntree=100, verbose=F, trControl=fitControl)
# randomForest----
fitControl <- trainControl(method='repeatedcv', number=10, repeats=3)
rf_fit <- train(survived ~ ., data=titanic.train,
preProcess = c("pca"),
method='rf', ntree=100, verbose=F, trControl=fitControl)
# var importance----
varImp(m)
printcp(m)
summary(m)
# ctree :: 조건부 추론나무
# 과적합, 변수선택 편중 문제 해결
m <- ctree(Species ~ ., data=iris)
plot(m)
levels(m)
levels(iris$Species)
yhat <- predict(m, newdata = iris, type = 'response')
head(yhat)
# left or not with hr dataset
hr <- read_csv("data/hr_comma_sep.csv")
colnames(hr) <- tolower(colnames(hr)); head(hr)
colnames(hr) <- tolower(colnames(hr)); glimpse(hr)
table(hr$left)
table(hr$sales)
table(hr$salary)
# splitting dataset----
set.seed(2018)
n <- nrow(hr)
idx <- 1:n
training_idx <- sample(idx, n * .6)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .2)
test_idx <- setdiff(idx, validate_idx)
hr.train <- hr[training_idx, ]
hr.validation <- hr[validate_idx, ]
hr.test <- hr[test_idx, ]
hr.train
# factorise
rf <- randomForest(as.factor(left) ~., hr.train %>%
mutate(salary = as.factor(salary),
sales = as.factor(sales)))
# select model
importance(rf)
# select model
order(importance(rf), desc=T)
# select model
order(importance(rf), descreasing=T)
# select model
importance(rf)
# select model
class(importance(rf))
as.data.frame(importance(rf)) %>%
arrange(MeanDecreaseGini, desc=T)
as.data.frame(importance(rf)) %>%
arrange(MeanDecreaseGini, desc)
as.data.frame(importance(rf))
as.data.frame(importance(rf)) %>%
arrange(MeanDecreaseGini)
as.data.frame(importance(rf)) %>%
arrange(desc(MeanDecreaseGini))
as.data.frame(importance(rf))
as.data.frame(importance(rf)) %>%
arrange(desc(MeanDecreaseGini))
# select model
importance(rf)
# select model
importance(rf)[, 1]
data.frame(var=rownames(importance(rf)), gini_desc=importance(rf)[, 1])
data.frame(var=rownames(importance(rf)), gini_desc=importance(rf)[, 1]) -> var_imp_df
var_imp_df %>%
arrange(desc(gini_desc))
# select model
importance(rf)
importance(rf)[, 1]
data.frame(var=rownames(importance(rf)), gini_desc=importance(rf)[, 1]) -> var_imp_df
var_imp_df %>%
arrange(desc(gini_desc))
plot(rf)
varImpPlot(rf)
# predict----
yhat <- predict(rf, newdata=hr.test, type='prob')
colnames(hr.train)
colnamse(hr.test)
colnames(hr.test)
# predict----
yhat <- predict(rf, newdata=hr.test %>%
mutate(left = as.factor(left),
salary = as.factor(salary),
sales = as.factor(sales)), type='prob')
# predict----
yhat <- predict(rf, newdata=hr.validation %>%
mutate(left = as.factor(left),
salary = as.factor(salary),
sales = as.factor(sales)), type='prob')
yhat
# predict----
yhat <- predict(rf, newdata=hr.validation %>%
mutate(left = as.factor(left),
salary = as.factor(salary),
sales = as.factor(sales)), type='prob')[, 1]
yhat
# evaluaing----
y_obs <- hr.test$left
y_obs
# evaluaing----
y_obs <- hr.test$left; class(y_obs)
pred <- prediction(yhat, y_obs)
yhat
pred <- prediction(yhat, y_obs)
class(yhat)
as.data.frame(predict(rf, newdata=hr.validation))
# evaluaing----
y_obs <- hr.validation$left
pred <- prediction(yhat, y_obs)
pred
plot(performance(pred, 'tpr', 'fpr'))
# predict----
yhat <- predict(rf, newdata=hr.validation %>%
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='prob') # left 인 경우만 선택하기 위해
yhat
yhat[1:5, ]
y_obs[1:5, ]
y_obs[1:5]
# predict----
yhat <- predict(rf, newdata=hr.validation %>%
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='prob')[, '1'] # left 인 경우만 선택하기 위해
yhat[1:5, ]
# evaluaing----
y_obs <- hr.validation$left
pred <- prediction(yhat, y_obs)
plot(performance(pred, 'tpr', 'fpr'))
yhat[1:5]
evaluaing----
y_obs <- hr.validation$left
# evaluaing----
y_obs <- hr.validation$left
# evaluaing----
y_obs <- hr.validation$left; y_obs[1:5]
pred <- prediction(yhat, y_obs)
plot(performance(pred, 'tpr', 'fpr'))
abline(0, 1)
plot(performance(pred, 'acc'))
performance(pred, 'auc')@y.values[[1]]
hr_glm_full <- glm(left ~., data=hr.train, family = binomial); summary(hr_glm_full)
perf_glm <- performance(hr_glm_full, 'tpr', 'fpr')
yhat_glm <- predict(hr_glm_full, newdata = hr.validation,
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='prob')[, '1'] # left 인 경우만 선택하기 위해
yhat_glm <- predict(hr_glm_full, newdata = hr.validation,
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='response')[, '1'] # left 인 경우만 선택하기 위해
head(hr.validation)
yhat_glm <- predict(hr_glm_full, newdata = hr.validation,
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='response')
yhat_glm <- predict(hr_glm_full, newdata = hr.validation,  type='response')
y_obs <- hr.validation$left; y_obs[1:5]
pred_lm <- prediction(yhat_glm, y_obs)
perf_lm <- performance(pred_lm, 'tpr', 'fpr')
## rf model
yhat_rf <- predict(rf, newdata=hr.validation %>%
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='prob')[, '1']
pred_rf <- prediction(yhat_rf, as.factor(y_obs))
perf_rf <- performance(pred_rf, 'tpr', 'fpr')
## glm model
hr_glm_full <- glm(left ~., data=hr.train, family = binomial); summary(hr_glm_full)
yhat_glm <- predict(hr_glm_full, newdata = hr.validation,  type='response')
y_obs <- hr.validation$left
pred_glm <- prediction(yhat_glm, y_obs)
perf_glm <- performance(pred_glm, 'tpr', 'fpr')
## rf model
yhat_rf <- predict(rf, newdata=hr.validation %>%
mutate(left = as.factor(left), # 자료혐을 training과 같게
salary = as.factor(salary), # 자료혐을 training과 같게
sales = as.factor(sales)), type='prob')[, '1']
pred_rf <- prediction(yhat_rf, as.factor(y_obs))
perf_rf <- performance(pred_rf, 'tpr', 'fpr')
## comparison betweem perf_glm, perf_rm
plot(perf_glm)
plot(perf_rf, add=T, color='red')
plot(perf_rf, add=T, col='red')
abline(0, 1, col='blue')
### 'auc' daya.frame
data.frame(method=c('glm', 'rf'),
auc = c(performance(pred_glm, 'auc')@y.values[[1]],
performance(pre_rf, 'rf')@y.values[[1]]))
### 'auc' daya.frame
data.frame(method=c('glm', 'rf'),
auc = c(performance(pred_glm, 'auc')@y.values[[1]],
performance(pred_rf, 'rf')@y.values[[1]]))
### 'auc' daya.frame
data.frame(method=c('glm', 'rf'),
auc = c(performance(pred_glm, 'auc')@y.values[[1]],
performance(pred_rf, 'aux')@y.values[[1]]))
### 'auc' daya.frame
data.frame(method=c('glm', 'rf'),
auc = c(performance(pred_glm, 'auc')@y.values[[1]],
performance(pred_rf, 'auc')@y.values[[1]]))
install.packages('tm')
install.packages('rJava')
library(rJava)
install.packages('KoNLP')
library(LKoNLP)
library(KoNLP)
userSejongDic()
useSejongDic()
install.packages('SnowballC')
install.packages('slam')
library(SnowballC)
library(slam)
Packages <- c('tidyverse', 'data.table', 'reshape2', 'caret', 'rpart', 'GGally', 'ROCR', 'party',
'randomForest', 'rJava', '')
lapply(Packages, library, character.only=T)
Packages <- c('tidyverse', 'data.table', 'reshape2', 'caret', 'rpart', 'GGally', 'ROCR', 'party',
'randomForest')
lapply(Packages, library, character.only=T)
options(mc.cores=1)
library(readr)
tvprograms <- read_delim("data/tvprograms.txt",
"\t", escape_double = FALSE, trim_ws = TRUE)
head(trprograms )
library(readr)
tvprograms <- read_delim("data/tvprograms.txt",
"\t", escape_double = FALSE, trim_ws = TRUE)
View(tvprograms)
head(trprograms)
head(tvprograms)
tvprograms <- read.table('./data/tvprograms.txt', header=T, stringsAsFactors = F)
tvprograms <- read.table('./data/tvprograms.txt', header=T, stringsAsFactors = F, fileEncoding = 'tf-8')
tvprograms <- read.table('./data/tvprograms.txt', header=T, stringsAsFactors = F, fileEncoding = 'utf-8')
head(tvprograms)
readLines('./data/tvprograms.txt')
readLines('./data/tvprograms.txt', encoding = 'utf-8')
head(tvprograms)
tvprograms <- read_delim("data/tvprograms.txt",
"\t", escape_double = FALSE, trim_ws = TRUE)
head(tvprograms)
library(readr)
tvprograms <- read_delim("data/tvprograms.txt",
"\t", escape_double = FALSE, trim_ws = TRUE)
View(tvprograms)
tvprograms <- read_delim("data/tvprograms.txt", "\t", escape_double = FALSE, trim_ws = TRUE)
head(tvprograms)
tvpro <- read_delim("data/tvprograms.txt", "\t", escape_double = FALSE, trim_ws = TRUE)
head(tvpro)
tvpro$title[is.na(tvpro$titla)]
head(tvpro, 2)
head(tvpro, 2)$title
head(tvpro, 2)$contents
head(tvpro, 2)$date
# remove punctuation
gsub('[[:punc:]]+', "", tvpro[, c(2, 3)])
# remove punctuation
gsub('[[:punct:]]+', "", tvpro[, c(2, 3)])
# remove punctuation
tvpro$title <- gsub('[[:punct:]]+', "", tvpro$title)
tvpro$contents <- gsub('[[:punct:]]+', "", tvpro$contents)
head(tvpro, 2)
d <- as.character(doc)
# ko.words() 함수 만들기----
ko.words <- function(doc) {
d <- as.character(doc)
extractNoun(d)
}
rm(list_rm())
rm(list = ls())
gc()
